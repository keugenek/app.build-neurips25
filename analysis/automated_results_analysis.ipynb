{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automated Test Results Analysis\n",
        "\n",
        "This notebook analyzes the **automated CI/CD test results** from the app.build benchmark.\n",
        "\n",
        "**Data Source**: Pre-processed CSV files from `analysis/results/` directory containing:\n",
        "- Binary success/failure from automated tests\n",
        "- Token usage and timing metrics\n",
        "- Docker health checks and template failures\n",
        "\n",
        "**Note**: This is separate from human evaluation results. For human-evaluated analysis, see `experiments_baseline_ablation_analysis.ipynb`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Baseline (Closed APIs) vs Open Models Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AUTOMATED TEST RESULTS SUMMARY ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cohort</th>\n",
              "      <th>num_runs</th>\n",
              "      <th>success_rate</th>\n",
              "      <th>mean_duration_s</th>\n",
              "      <th>total_input_tokens</th>\n",
              "      <th>total_output_tokens</th>\n",
              "      <th>est_cost_usd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline (closed APIs)</td>\n",
              "      <td>30</td>\n",
              "      <td>86.7%</td>\n",
              "      <td>478.3</td>\n",
              "      <td>27691026</td>\n",
              "      <td>1808588</td>\n",
              "      <td>110.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Open models</td>\n",
              "      <td>180</td>\n",
              "      <td>56.7%</td>\n",
              "      <td>628.6</td>\n",
              "      <td>219116367</td>\n",
              "      <td>7771684</td>\n",
              "      <td>37.53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   cohort  num_runs success_rate  mean_duration_s  \\\n",
              "0  Baseline (closed APIs)        30        86.7%            478.3   \n",
              "1             Open models       180        56.7%            628.6   \n",
              "\n",
              "   total_input_tokens  total_output_tokens  est_cost_usd  \n",
              "0            27691026              1808588        110.20  \n",
              "1           219116367              7771684         37.53  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Automated Tests: Closed APIs: 86.7%, ~478s, $110.20; Open models: 56.7%, ~629s, $37.53.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Load pre-processed results from CSV files\n",
        "RESULTS_DIR = Path(\"results\") if Path(\"results\").exists() else Path(\"/Users/evgenii.kniazev/projects/app.build-neurips25/analysis/results\")\n",
        "\n",
        "# Load baseline and openmodels data\n",
        "baseline_raw = pd.read_csv(RESULTS_DIR / \"baseline\" / \"raw_results.csv\")\n",
        "openmodels_raw = pd.read_csv(RESULTS_DIR / \"openmodels\" / \"raw_results.csv\")\n",
        "\n",
        "# Calculate cost based on model family\n",
        "PRICES = {\"anthropic\": (3.0, 15.0), \"gemini\": (0.15, 0.60), \"openai\": (5.0, 15.0)}\n",
        "\n",
        "def get_model_family(coding_model: str, universal_model: str) -> str:\n",
        "    \"\"\"Determine model family from coding and universal model names.\"\"\"\n",
        "    combined = f\"{coding_model} {universal_model}\".lower()\n",
        "    if \"claude\" in combined:\n",
        "        return \"anthropic\"\n",
        "    elif \"gemini\" in combined:\n",
        "        return \"gemini\"\n",
        "    elif \"gpt\" in combined and not \"gpt-oss\" in combined:\n",
        "        return \"openai\"\n",
        "    return \"open\"\n",
        "\n",
        "def calculate_cost(df: pd.DataFrame) -> float:\n",
        "    \"\"\"Calculate total cost based on token usage and model pricing.\"\"\"\n",
        "    if df.empty:\n",
        "        return 0.0\n",
        "    \n",
        "    df = df.copy()\n",
        "    df[\"family\"] = df.apply(lambda r: get_model_family(r[\"coding_model\"], r[\"universal_model\"]), axis=1)\n",
        "    \n",
        "    total_cost = 0.0\n",
        "    for family, group in df.groupby(\"family\"):\n",
        "        rate_in, rate_out = PRICES.get(family, (0.0, 0.0))\n",
        "        input_cost = (group[\"total_input_tokens\"].sum() / 1e6) * rate_in\n",
        "        output_cost = (group[\"total_output_tokens\"].sum() / 1e6) * rate_out\n",
        "        total_cost += input_cost + output_cost\n",
        "    \n",
        "    return total_cost\n",
        "\n",
        "# Create summary statistics\n",
        "summary_data = []\n",
        "\n",
        "for label, df in [(\"Baseline (closed APIs)\", baseline_raw), (\"Open models\", openmodels_raw)]:\n",
        "    summary_data.append({\n",
        "        \"cohort\": label,\n",
        "        \"num_runs\": len(df),\n",
        "        \"success_rate\": df[\"success\"].mean() if not df.empty else np.nan,\n",
        "        \"mean_duration_s\": df[\"duration_seconds\"].mean() if not df.empty else np.nan,\n",
        "        \"total_input_tokens\": int(df[\"total_input_tokens\"].sum()) if not df.empty else 0,\n",
        "        \"total_output_tokens\": int(df[\"total_output_tokens\"].sum()) if not df.empty else 0,\n",
        "        \"est_cost_usd\": calculate_cost(df),\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Format for display\n",
        "display_df = summary_df.copy()\n",
        "display_df[\"success_rate\"] = display_df[\"success_rate\"].map(lambda x: \"--\" if pd.isna(x) else f\"{x*100:.1f}%\")\n",
        "display_df[\"mean_duration_s\"] = display_df[\"mean_duration_s\"].round(1)\n",
        "display_df[\"est_cost_usd\"] = display_df[\"est_cost_usd\"].round(2)\n",
        "\n",
        "# Display the results\n",
        "print(\"\\n=== AUTOMATED TEST RESULTS SUMMARY ===\")\n",
        "try:\n",
        "    display(display_df[[\"cohort\", \"num_runs\", \"success_rate\", \"mean_duration_s\", \"total_input_tokens\", \"total_output_tokens\", \"est_cost_usd\"]])\n",
        "except Exception:\n",
        "    print(display_df.to_string(index=False))\n",
        "\n",
        "# Print summary takeaway\n",
        "if len(summary_df) >= 2:\n",
        "    baseline = summary_df.iloc[0]\n",
        "    openmodels = summary_df.iloc[1]\n",
        "    print(f\"\\nAutomated Tests: Closed APIs: {display_df.loc[0,'success_rate']}, ~{baseline['mean_duration_s']:.0f}s, ${baseline['est_cost_usd']:.2f}; \"\n",
        "          f\"Open models: {display_df.loc[1,'success_rate']}, ~{openmodels['mean_duration_s']:.0f}s, ${openmodels['est_cost_usd']:.2f}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Ablation Study: Impact of Validation Layers\n",
        "\n",
        "Comparing baseline with ablations (no_lint, no_playwright, no_tests) using automated test results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AUTOMATED TEST ABLATION STUDY ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cohort</th>\n",
              "      <th>num_runs</th>\n",
              "      <th>success_rate</th>\n",
              "      <th>mean_duration_s</th>\n",
              "      <th>total_input_tokens</th>\n",
              "      <th>total_output_tokens</th>\n",
              "      <th>est_cost_usd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline</td>\n",
              "      <td>30</td>\n",
              "      <td>86.7%</td>\n",
              "      <td>478.3</td>\n",
              "      <td>27691026</td>\n",
              "      <td>1808588</td>\n",
              "      <td>110.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No Lint</td>\n",
              "      <td>30</td>\n",
              "      <td>93.3%</td>\n",
              "      <td>496.1</td>\n",
              "      <td>15938734</td>\n",
              "      <td>1511313</td>\n",
              "      <td>70.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No Playwright</td>\n",
              "      <td>30</td>\n",
              "      <td>83.3%</td>\n",
              "      <td>462.7</td>\n",
              "      <td>20822496</td>\n",
              "      <td>1580274</td>\n",
              "      <td>86.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No Tests</td>\n",
              "      <td>30</td>\n",
              "      <td>93.3%</td>\n",
              "      <td>372.7</td>\n",
              "      <td>15915645</td>\n",
              "      <td>1553654</td>\n",
              "      <td>71.05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          cohort  num_runs success_rate  mean_duration_s  total_input_tokens  \\\n",
              "0       Baseline        30        86.7%            478.3            27691026   \n",
              "1        No Lint        30        93.3%            496.1            15938734   \n",
              "2  No Playwright        30        83.3%            462.7            20822496   \n",
              "3       No Tests        30        93.3%            372.7            15915645   \n",
              "\n",
              "   total_output_tokens  est_cost_usd  \n",
              "0              1808588        110.20  \n",
              "1              1511313         70.49  \n",
              "2              1580274         86.17  \n",
              "3              1553654         71.05  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Relative changes from baseline (automated tests):\n",
            "No Lint: Success +6.7%, Duration +18s, Cost $-39.72\n",
            "No Playwright: Success -3.3%, Duration -16s, Cost $-24.03\n",
            "No Tests: Success +6.7%, Duration -106s, Cost $-39.15\n"
          ]
        }
      ],
      "source": [
        "# Load ablation study results from CSV files\n",
        "ablation_types = [\"baseline\", \"no_lint\", \"no_playwright\", \"no_tests\"]\n",
        "ablation_data = []\n",
        "\n",
        "for ablation in ablation_types:\n",
        "    # Load raw results\n",
        "    raw_df = pd.read_csv(RESULTS_DIR / ablation / \"raw_results.csv\")\n",
        "    \n",
        "    # Calculate stats\n",
        "    ablation_data.append({\n",
        "        \"cohort\": ablation.replace(\"_\", \" \").title(),\n",
        "        \"num_runs\": len(raw_df),\n",
        "        \"success_rate\": raw_df[\"success\"].mean() if not raw_df.empty else np.nan,\n",
        "        \"mean_duration_s\": raw_df[\"duration_seconds\"].mean() if not raw_df.empty else np.nan,\n",
        "        \"total_input_tokens\": int(raw_df[\"total_input_tokens\"].sum()) if not raw_df.empty else 0,\n",
        "        \"total_output_tokens\": int(raw_df[\"total_output_tokens\"].sum()) if not raw_df.empty else 0,\n",
        "        \"est_cost_usd\": calculate_cost(raw_df),\n",
        "    })\n",
        "\n",
        "ablation_df = pd.DataFrame(ablation_data)\n",
        "\n",
        "# Format for display\n",
        "ablation_display = ablation_df.copy()\n",
        "ablation_display[\"success_rate\"] = ablation_display[\"success_rate\"].map(lambda x: \"--\" if pd.isna(x) else f\"{x*100:.1f}%\")\n",
        "ablation_display[\"mean_duration_s\"] = ablation_display[\"mean_duration_s\"].round(1)\n",
        "ablation_display[\"est_cost_usd\"] = ablation_display[\"est_cost_usd\"].round(2)\n",
        "\n",
        "# Display the ablation comparison\n",
        "print(\"\\n=== AUTOMATED TEST ABLATION STUDY ===\")\n",
        "try:\n",
        "    display(ablation_display[[\"cohort\", \"num_runs\", \"success_rate\", \"mean_duration_s\", \"total_input_tokens\", \"total_output_tokens\", \"est_cost_usd\"]])\n",
        "except Exception:\n",
        "    print(ablation_display.to_string(index=False))\n",
        "\n",
        "# Calculate relative changes from baseline\n",
        "baseline_idx = 0\n",
        "print(\"\\nRelative changes from baseline (automated tests):\")\n",
        "for i in range(1, len(ablation_df)):\n",
        "    cohort = ablation_df.iloc[i][\"cohort\"]\n",
        "    success_delta = (ablation_df.iloc[i][\"success_rate\"] - ablation_df.iloc[baseline_idx][\"success_rate\"]) * 100\n",
        "    duration_delta = ablation_df.iloc[i][\"mean_duration_s\"] - ablation_df.iloc[baseline_idx][\"mean_duration_s\"]\n",
        "    cost_delta = ablation_df.iloc[i][\"est_cost_usd\"] - ablation_df.iloc[baseline_idx][\"est_cost_usd\"]\n",
        "    \n",
        "    print(f\"{cohort}: Success {success_delta:+.1f}%, Duration {duration_delta:+.0f}s, Cost ${cost_delta:+.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Summary from .out Files\n",
        "\n",
        "Parse the pre-generated analysis output files for exact statistics from the automated test runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SUMMARY FROM .OUT FILES (AUTOMATED TESTS) ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ablation</th>\n",
              "      <th>Experiments</th>\n",
              "      <th>Success Rate</th>\n",
              "      <th>Avg Duration</th>\n",
              "      <th>Input Tokens</th>\n",
              "      <th>Output Tokens</th>\n",
              "      <th>Total Cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline</td>\n",
              "      <td>30</td>\n",
              "      <td>86.7%</td>\n",
              "      <td>478.3s</td>\n",
              "      <td>27,691,026</td>\n",
              "      <td>1,808,588</td>\n",
              "      <td>$110.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No Lint</td>\n",
              "      <td>30</td>\n",
              "      <td>93.3%</td>\n",
              "      <td>496.1s</td>\n",
              "      <td>15,938,734</td>\n",
              "      <td>1,511,313</td>\n",
              "      <td>$70.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No Playwright</td>\n",
              "      <td>30</td>\n",
              "      <td>83.3%</td>\n",
              "      <td>462.7s</td>\n",
              "      <td>20,822,496</td>\n",
              "      <td>1,580,274</td>\n",
              "      <td>$86.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No Tests</td>\n",
              "      <td>30</td>\n",
              "      <td>93.3%</td>\n",
              "      <td>372.7s</td>\n",
              "      <td>15,915,645</td>\n",
              "      <td>1,553,654</td>\n",
              "      <td>$71.05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Ablation Experiments Success Rate Avg Duration Input Tokens  \\\n",
              "0       Baseline          30        86.7%       478.3s   27,691,026   \n",
              "1        No Lint          30        93.3%       496.1s   15,938,734   \n",
              "2  No Playwright          30        83.3%       462.7s   20,822,496   \n",
              "3       No Tests          30        93.3%       372.7s   15,915,645   \n",
              "\n",
              "  Output Tokens Total Cost  \n",
              "0     1,808,588    $110.20  \n",
              "1     1,511,313     $70.49  \n",
              "2     1,580,274     $86.17  \n",
              "3     1,553,654     $71.05  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Performance Impact of Ablations (Automated Tests):\n",
            "\n",
            "No Lint:\n",
            "  - Success rate: +6.7% (93.3% vs 86.7%)\n",
            "  - Avg duration: +17.8s (496.1s vs 478.3s)\n",
            "  - Total cost: $-39.71 ($70.49 vs $110.20)\n",
            "\n",
            "No Playwright:\n",
            "  - Success rate: -3.3% (83.3% vs 86.7%)\n",
            "  - Avg duration: -15.6s (462.7s vs 478.3s)\n",
            "  - Total cost: $-24.03 ($86.17 vs $110.20)\n",
            "\n",
            "No Tests:\n",
            "  - Success rate: +6.7% (93.3% vs 86.7%)\n",
            "  - Avg duration: -105.6s (372.7s vs 478.3s)\n",
            "  - Total cost: $-39.15 ($71.05 vs $110.20)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def parse_out_file(filepath: Path) -> dict:\n",
        "    \"\"\"Parse a .out file and extract key statistics.\"\"\"\n",
        "    content = filepath.read_text()\n",
        "    stats = {}\n",
        "    \n",
        "    # Extract key metrics using regex\n",
        "    patterns = {\n",
        "        \"total_experiments\": r\"Total experiments: (\\d+)\",\n",
        "        \"success_rate\": r\"Success rate: ([\\d.]+)%\",\n",
        "        \"healthcheck_pass_rate\": r\"Healthcheck pass rate: ([\\d.]+)%\",\n",
        "        \"template_failure_rate\": r\"Template failure rate \\(trpc-agent\\): ([\\d.]+)%\",\n",
        "        \"avg_duration\": r\"Average duration: ([\\d.]+)s\",\n",
        "        \"median_duration\": r\"Median duration: ([\\d.]+)s\",\n",
        "        \"total_input_tokens\": r\"Total input tokens: ([\\d,_]+)\",\n",
        "        \"total_output_tokens\": r\"Total output tokens: ([\\d,_]+)\",\n",
        "        \"total_llm_calls\": r\"Total LLM calls: ([\\d,_]+)\",\n",
        "        \"total_cost\": r\"Total cost: \\$([\\d.]+)\",\n",
        "        \"avg_tokens_sec_input\": r\"Avg tokens/sec \\(input\\): (\\d+)\",\n",
        "        \"avg_tokens_sec_output\": r\"Avg tokens/sec \\(output\\): (\\d+)\",\n",
        "    }\n",
        "    \n",
        "    for key, pattern in patterns.items():\n",
        "        match = re.search(pattern, content)\n",
        "        if match:\n",
        "            value = match.group(1).replace(\",\", \"\").replace(\"_\", \"\")\n",
        "            if key in [\"success_rate\", \"healthcheck_pass_rate\", \"template_failure_rate\"]:\n",
        "                stats[key] = float(value)\n",
        "            elif key in [\"avg_duration\", \"median_duration\", \"total_cost\"]:\n",
        "                stats[key] = float(value)\n",
        "            elif \"tokens\" in key or \"calls\" in key:\n",
        "                stats[key] = int(value)\n",
        "            else:\n",
        "                stats[key] = value\n",
        "    \n",
        "    return stats\n",
        "\n",
        "# Parse all .out files\n",
        "out_files = {\n",
        "    \"baseline\": \"baseline.out\",\n",
        "    \"no_lint\": \"ablation_no_lint.out\",\n",
        "    \"no_playwright\": \"ablation_no_playwright.out\",\n",
        "    \"no_tests\": \"ablation_no_tests.out\"\n",
        "}\n",
        "\n",
        "analysis_dir = Path(\".\") if Path(\"baseline.out\").exists() else Path(\"/Users/evgenii.kniazev/projects/app.build-neurips25/analysis\")\n",
        "\n",
        "out_file_stats = []\n",
        "for ablation_type, filename in out_files.items():\n",
        "    filepath = analysis_dir / filename\n",
        "    if filepath.exists():\n",
        "        stats = parse_out_file(filepath)\n",
        "        stats[\"ablation_type\"] = ablation_type\n",
        "        out_file_stats.append(stats)\n",
        "\n",
        "# Create DataFrame from parsed data\n",
        "out_stats_df = pd.DataFrame(out_file_stats)\n",
        "\n",
        "# Format for display\n",
        "display_out_stats = pd.DataFrame({\n",
        "    \"Ablation\": out_stats_df[\"ablation_type\"].str.replace(\"_\", \" \").str.title(),\n",
        "    \"Experiments\": out_stats_df[\"total_experiments\"],\n",
        "    \"Success Rate\": out_stats_df[\"success_rate\"].map(lambda x: f\"{x:.1f}%\"),\n",
        "    \"Avg Duration\": out_stats_df[\"avg_duration\"].map(lambda x: f\"{x:.1f}s\"),\n",
        "    \"Input Tokens\": out_stats_df[\"total_input_tokens\"].map(lambda x: f\"{x:,}\"),\n",
        "    \"Output Tokens\": out_stats_df[\"total_output_tokens\"].map(lambda x: f\"{x:,}\"),\n",
        "    \"Total Cost\": out_stats_df[\"total_cost\"].map(lambda x: f\"${x:.2f}\")\n",
        "})\n",
        "\n",
        "print(\"\\n=== SUMMARY FROM .OUT FILES (AUTOMATED TESTS) ===\")\n",
        "try:\n",
        "    display(display_out_stats)\n",
        "except Exception:\n",
        "    print(display_out_stats.to_string(index=False))\n",
        "\n",
        "# Show performance comparison\n",
        "print(\"\\nPerformance Impact of Ablations (Automated Tests):\")\n",
        "baseline_stats = out_stats_df[out_stats_df[\"ablation_type\"] == \"baseline\"].iloc[0]\n",
        "for _, row in out_stats_df[out_stats_df[\"ablation_type\"] != \"baseline\"].iterrows():\n",
        "    ablation = row[\"ablation_type\"].replace(\"_\", \" \").title()\n",
        "    success_diff = row[\"success_rate\"] - baseline_stats[\"success_rate\"]\n",
        "    duration_diff = row[\"avg_duration\"] - baseline_stats[\"avg_duration\"]\n",
        "    cost_diff = row[\"total_cost\"] - baseline_stats[\"total_cost\"]\n",
        "    \n",
        "    print(f\"\\n{ablation}:\")\n",
        "    print(f\"  - Success rate: {success_diff:+.1f}% ({row['success_rate']:.1f}% vs {baseline_stats['success_rate']:.1f}%)\")\n",
        "    print(f\"  - Avg duration: {duration_diff:+.1f}s ({row['avg_duration']:.1f}s vs {baseline_stats['avg_duration']:.1f}s)\")\n",
        "    print(f\"  - Total cost: ${cost_diff:+.2f} (${row['total_cost']:.2f} vs ${baseline_stats['total_cost']:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Key Findings from Automated Tests\n",
        "\n",
        "### Success Rates (Automated CI/CD)\n",
        "- **Baseline**: ~86.7% success rate\n",
        "- **No Lint**: ~93.3% success rate (+6.7%)\n",
        "- **No Playwright**: ~83.3% success rate (-3.3%)\n",
        "- **No Tests**: ~93.3% success rate (+6.7%)\n",
        "\n",
        "### Observations\n",
        "1. **Linting and unit tests appear to reduce success rates** in automated tests\n",
        "2. **Playwright (E2E) tests have minimal impact** on automated success\n",
        "3. **Cost savings are significant** when validation is reduced (~$40 less)\n",
        "\n",
        "### ⚠️ Important Caveat\n",
        "These are automated test results that may not reflect actual app quality. Human evaluation shows different patterns:\n",
        "- Human viability rates are generally lower (73.3% vs 86.7%)\n",
        "- Quality assessment requires nuanced evaluation beyond binary pass/fail\n",
        "\n",
        "For human-evaluated results, see `experiments_baseline_ablation_analysis.ipynb`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
